# ğŸš€ GuÃ­a de Inicio RÃ¡pido - Labortrovilo Backend
# Rapida Starta Gvidilo - Labortrovilo Backend
# Quick Start Guide - Labortrovilo Backend

## ğŸ“ Estructura del Proyecto / Projekta Strukturo

```
labortrovilo/
â”œâ”€â”€ src/                       # ğŸ¯ CÃ“DIGO PRINCIPAL / ÄˆEFA KODO
â”‚   â”œâ”€â”€ __init__.py           # InicializaciÃ³n del paquete
â”‚   â”œâ”€â”€ models.py             # ğŸ—„ï¸ Modelos SQLAlchemy (Jobs, Companies)
â”‚   â”œâ”€â”€ schemas.py            # âœ… Esquemas Pydantic (validaciÃ³n)
â”‚   â”œâ”€â”€ database.py           # ğŸ’¾ ConfiguraciÃ³n de base de datos
â”‚   â””â”€â”€ scraper_engine.py     # ğŸ¤– Motor de scraping principal
â”‚
â”œâ”€â”€ logs/                      # ğŸ“ Logs del scraper
â”œâ”€â”€ config.py                  # âš™ï¸ ConfiguraciÃ³n centralizada
â”œâ”€â”€ test_scraper.py            # ğŸ§ª Script de pruebas
â””â”€â”€ labortrovilo.db            # ğŸ—„ï¸ Base de datos SQLite
```

## ğŸš€ InstalaciÃ³n y Setup

### 1. Activar entorno virtual
```bash
venv\Scripts\activate
```

### 2. Verificar dependencias
```bash
pip list | findstr "playwright pydantic sqlalchemy"
```

### 3. Ejecutar test bÃ¡sico
```bash
python test_scraper.py
```

## ğŸ¯ Uso del Scraper

### OpciÃ³n 1: Usar el script de test
```python
python test_scraper.py
# Selecciona opciÃ³n 1 para test completo
```

### OpciÃ³n 2: Importar y usar directamente
```python
import asyncio
from src.scraper_engine import LabortroviloScraper
from src.database import init_db

async def main():
    # Inicializar BD
    init_db()
    
    # Crear scraper
    scraper = LabortroviloScraper(headless=True)
    
    try:
        await scraper.initialize()
        
        # Scrapear una URL
        result = await scraper.scrape_job("https://example.com/job")
        
        print(f"Success: {result.success}")
        if result.job_data:
            print(f"Title: {result.job_data.title}")
            print(f"Urgency: {result.job_data.hiring_urgency_score}")
        
    finally:
        await scraper.close()

asyncio.run(main())
```

## ğŸ”§ PersonalizaciÃ³n por ATS

Los selectores CSS en `scraper_engine.py` son genÃ©ricos. Para cada ATS, personaliza:

```python
# En extract_job_data():

# Para Greenhouse:
title = await page.locator('.app-title').text_content()
company = await page.locator('.company-name').text_content()

# Para Lever:
title = await page.locator('h2[data-qa="job-title"]').text_content()
company = await page.locator('.main-footer-text a').text_content()

# Para Workday:
title = await page.locator('h3[data-automation-id="jobTitle"]').text_content()
```

## ğŸ“Š Campos Diferenciadores

### ğŸ¯ hiring_urgency_score (0-100)
Calcula automÃ¡ticamente basÃ¡ndose en:
- Palabras clave de urgencia (urgent, immediate, ASAP)
- Fecha de publicaciÃ³n reciente
- Indicadores en tÃ­tulo (senior, lead)

### ğŸ¯ is_it_niche (boolean)
Detecta nichos especializados:
- blockchain, web3, crypto
- quantum computing
- machine learning, AI
- bioinformatics
- embedded systems, IoT

## ğŸ› ï¸ Comandos Ãštiles

### Ver estadÃ­sticas de BD
```python
from src.database import db_manager
stats = db_manager.get_stats()
print(stats)
```

### Verificar salud de BD
```python
from src.database import db_manager
health = db_manager.health_check()
print(f"DB Health: {health}")
```

### Ver logs en tiempo real
```bash
Get-Content -Path "logs\scraper.log" -Tail 20 -Wait
```

## ğŸ“ Logging

Los logs se guardan en:
- `logs/scraper.log` - Todos los eventos
- TambiÃ©n se muestran en consola

Niveles de log:
- INFO: Operaciones normales
- WARNING: Situaciones que requieren atenciÃ³n
- ERROR: Errores capturados pero manejados
- CRITICAL: Errores graves

## âš ï¸ Notas Importantes

1. **URLs de Prueba**: Los selectores genÃ©ricos necesitan personalizaciÃ³n por ATS
2. **Rate Limiting**: Incluye delays entre requests (configurable en config.py)
3. **Duplicados**: El sistema previene automÃ¡ticamente duplicados por URL
4. **Errores**: Los errores se registran pero NO detienen el scraper

## ğŸ” Siguiente Paso

Edita `test_scraper.py` y agrega URLs reales de ATS para probar:
```python
test_urls = [
    "https://boards.greenhouse.io/company/jobs/123456",
    "https://jobs.lever.co/company/job-id",
]
```

## ğŸ“š DocumentaciÃ³n Adicional

- `src/models.py`: Esquema completo de base de datos
- `src/schemas.py`: Todos los esquemas de validaciÃ³n
- `config.py`: Todas las opciones configurables
